{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 134
    },
    "colab_type": "code",
    "id": "7AnehLHnGAOv",
    "outputId": "6f00fc26-2e65-4837-e71b-e37b2045b84a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  adding: content/logss_model_dense/ (stored 0%)\n",
      "  adding: content/logss_model_dense/events.out.tfevents.1570487317.dea8a2611466 (deflated 92%)\n",
      "  adding: content/logss_model_dense/plugins/ (stored 0%)\n",
      "  adding: content/logss_model_dense/plugins/profile/ (stored 0%)\n",
      "  adding: content/logss_model_dense/plugins/profile/2019-10-07_22-28-53/ (stored 0%)\n",
      "  adding: content/logss_model_dense/plugins/profile/2019-10-07_22-28-53/local.trace (deflated 93%)\n",
      "  adding: content/logss_model_dense/events.out.tfevents.1570487333.dea8a2611466.profile-empty (deflated 5%)\n"
     ]
    }
   ],
   "source": [
    "!zip -r /content/file.zip /content/logss_model_dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XLa6Hd4qGRU0"
   },
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "files.download(\"/content/file.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wVIx_KIigxPV"
   },
   "outputs": [],
   "source": [
    "# import keras\n",
    "# from keras.datasets import cifar10\n",
    "# from keras.models import Model, Sequential\n",
    "# from keras.layers import Dense, Dropout, Flatten, Input, AveragePooling2D, merge, Activation\n",
    "# from keras.layers import Conv2D, MaxPooling2D, BatchNormalization\n",
    "# from keras.layers import Concatenate\n",
    "# from keras.optimizers import Adam\n",
    "from tensorflow.keras import models, layers\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import BatchNormalization, Activation, Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, TensorBoard, ReduceLROnPlateau "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UNHw6luQg3gc"
   },
   "outputs": [],
   "source": [
    "# this part will prevent tensorflow to allocate all the avaliable GPU Memory\n",
    "# backend\n",
    "import tensorflow as tf\n",
    "# from tensorflow import keras\n",
    "\n",
    "# from keras import backend as k\n",
    "\n",
    "# Don't pre-allocate memory; allocate as-needed\n",
    "# import tensorflow as tf\n",
    "# tf.config.gpu.set_per_process_memory_fraction(0.75)\n",
    "# tf.config.gpu.set_per_process_memory_growth(True)\n",
    "# config = tf.ConfigProto()\n",
    "# config.gpu_options.allow_growth = True\n",
    "\n",
    "# Create a session with the above options specified.\n",
    "# k.tensorflow_backend.set_session(tf.Session(config=config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mB7o3zu1g6eT"
   },
   "outputs": [],
   "source": [
    "# Load CIFAR10 Data\n",
    "num_classes = 10\n",
    "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
    "img_height, img_width, channel = X_train.shape[1],X_train.shape[2],X_train.shape[3]\n",
    "\n",
    "# convert to one hot encoing \n",
    "y_train = tf.keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, num_classes) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    },
    "colab_type": "code",
    "id": "14INOIwZtVGC",
    "outputId": "e4c547ed-6d76-4b45-cda6-ea27994d1bf1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 32, 32, 3)"
      ]
     },
     "execution_count": 7,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    },
    "colab_type": "code",
    "id": "LEVzuOHotVGI",
    "outputId": "4422d0fc-ed43-442c-883c-394c88846f1d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 32, 32, 3)"
      ]
     },
     "execution_count": 8,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ee-sge5Kg7vr"
   },
   "outputs": [],
   "source": [
    "# Dense Block\n",
    "compression = 0.5\n",
    "def denseblock(input, num_filter = 12, dropout_rate = 0.2):\n",
    "    global compression\n",
    "    temp = input\n",
    "    for _ in range(l): \n",
    "        BatchNorm = layers.BatchNormalization()(temp)\n",
    "        relu = layers.Activation('relu')(BatchNorm)\n",
    "        Conv2D_3_3 = layers.Conv2D(int(num_filter*compression), (3,3), use_bias=False ,padding='same')(relu)\n",
    "        if dropout_rate>0:\n",
    "            Conv2D_3_3 = layers.Dropout(dropout_rate)(Conv2D_3_3)\n",
    "        concat = layers.Concatenate(axis=-1)([temp,Conv2D_3_3])\n",
    "        \n",
    "        temp = concat\n",
    "        \n",
    "    return temp\n",
    "\n",
    "## transition Blosck\n",
    "def transition(input, num_filter = 12, dropout_rate = 0.2):\n",
    "    global compression\n",
    "    BatchNorm = layers.BatchNormalization()(input)\n",
    "    relu = layers.Activation('relu')(BatchNorm)\n",
    "    Conv2D_BottleNeck = layers.Conv2D(int(num_filter*compression), (1,1), use_bias=False ,padding='same')(relu)\n",
    "    if dropout_rate>0:\n",
    "         Conv2D_BottleNeck = layers.Dropout(dropout_rate)(Conv2D_BottleNeck)\n",
    "    avg = layers.AveragePooling2D(pool_size=(2,2))(Conv2D_BottleNeck)\n",
    "    return avg\n",
    "\n",
    "#output layer\n",
    "def output_layer(input):\n",
    "    global compression\n",
    "    BatchNorm = layers.BatchNormalization()(input)\n",
    "    relu = layers.Activation('relu')(BatchNorm)\n",
    "    AvgPooling = layers.AveragePooling2D(pool_size=(2,2))(relu)\n",
    "    Conv1 = layers.Conv2D(int(num_filter*compression), (1,1), use_bias=False ,padding='same')(AvgPooling)\n",
    "    Conv2 = layers.Conv2D(num_classes, (1,1) ,padding='valid')(Conv1)\n",
    "    Global_pool = layers.GlobalAveragePooling2D()(Conv2)\n",
    "    \n",
    "    output = layers.Activation('softmax')(Global_pool)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dsO_yGxcg5D8"
   },
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "batch_size = 128\n",
    "num_classes = 10\n",
    "epochs = 35\n",
    "l = 6\n",
    "num_filter = 64\n",
    "compression = 0.5\n",
    "dropout_rate = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 87
    },
    "colab_type": "code",
    "id": "anPCpQWhhGb7",
    "outputId": "a5a90c79-3242-449d-900d-439de80505ad"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    }
   ],
   "source": [
    "input = layers.Input(shape=(img_height, img_width, channel,))\n",
    "First_Conv2D = layers.Conv2D(num_filter, (3,3), use_bias=False ,padding='same')(input)\n",
    "\n",
    "First_Block = denseblock(First_Conv2D, num_filter, dropout_rate)\n",
    "First_Transition = transition(First_Block, num_filter, dropout_rate)\n",
    "\n",
    "Second_Block = denseblock(First_Transition, num_filter, dropout_rate)\n",
    "Second_Transition = transition(Second_Block, num_filter, dropout_rate)\n",
    "\n",
    "Third_Block = denseblock(Second_Transition, num_filter, dropout_rate)\n",
    "Third_Transition = transition(Third_Block, num_filter, dropout_rate)\n",
    "\n",
    "Last_Block = denseblock(Third_Transition,  num_filter, dropout_rate)\n",
    "output = output_layer(Last_Block)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Hoqh45qntVGX"
   },
   "outputs": [],
   "source": [
    "#https://arxiv.org/pdf/1608.06993.pdf\n",
    "# from IPython.display import IFrame, YouTubeVideo\n",
    "# YouTubeVideo(id='-W6y8xnd--U', width=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "1kFh7pdxhNtT",
    "outputId": "b7abe224-e39a-402f-f91c-be1bc34614f1",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 32, 32, 3)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 32, 32, 64)   1728        input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 32, 32, 64)   256         conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 32, 32, 64)   0           batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 32, 32, 32)   18432       activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 32, 32, 96)   0           conv2d[0][0]                     \n",
      "                                                                 conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 32, 32, 96)   384         concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 32, 32, 96)   0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 32, 32, 32)   27648       activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 32, 32, 128)  0           concatenate[0][0]                \n",
      "                                                                 conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 32, 32, 128)  512         concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 32, 32, 128)  0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 32, 32, 32)   36864       activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 32, 32, 160)  0           concatenate_1[0][0]              \n",
      "                                                                 conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 32, 32, 160)  640         concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 32, 32, 160)  0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 32, 32, 32)   46080       activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 32, 32, 192)  0           concatenate_2[0][0]              \n",
      "                                                                 conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 32, 32, 192)  768         concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 32, 32, 192)  0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 32, 32, 32)   55296       activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 32, 32, 224)  0           concatenate_3[0][0]              \n",
      "                                                                 conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 32, 32, 224)  896         concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 32, 32, 224)  0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 32, 32, 32)   64512       activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)     (None, 32, 32, 256)  0           concatenate_4[0][0]              \n",
      "                                                                 conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 32, 32, 256)  1024        concatenate_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 32, 32, 256)  0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 32, 32, 32)   8192        activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d (AveragePooli (None, 16, 16, 32)   0           conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 16, 16, 32)   128         average_pooling2d[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 16, 16, 32)   0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 16, 16, 32)   9216        activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)     (None, 16, 16, 64)   0           average_pooling2d[0][0]          \n",
      "                                                                 conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 16, 16, 64)   256         concatenate_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 16, 16, 64)   0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 16, 16, 32)   18432       activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_7 (Concatenate)     (None, 16, 16, 96)   0           concatenate_6[0][0]              \n",
      "                                                                 conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 16, 16, 96)   384         concatenate_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 16, 16, 96)   0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 16, 16, 32)   27648       activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_8 (Concatenate)     (None, 16, 16, 128)  0           concatenate_7[0][0]              \n",
      "                                                                 conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 16, 16, 128)  512         concatenate_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 16, 16, 128)  0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 16, 16, 32)   36864       activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_9 (Concatenate)     (None, 16, 16, 160)  0           concatenate_8[0][0]              \n",
      "                                                                 conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 16, 16, 160)  640         concatenate_9[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 16, 16, 160)  0           batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 16, 16, 32)   46080       activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_10 (Concatenate)    (None, 16, 16, 192)  0           concatenate_9[0][0]              \n",
      "                                                                 conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 16, 16, 192)  768         concatenate_10[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 16, 16, 192)  0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 16, 16, 32)   55296       activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_11 (Concatenate)    (None, 16, 16, 224)  0           concatenate_10[0][0]             \n",
      "                                                                 conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 16, 16, 224)  896         concatenate_11[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 16, 16, 224)  0           batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 16, 16, 32)   7168        activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_1 (AveragePoo (None, 8, 8, 32)     0           conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 8, 8, 32)     128         average_pooling2d_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 8, 8, 32)     0           batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 8, 8, 32)     9216        activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_12 (Concatenate)    (None, 8, 8, 64)     0           average_pooling2d_1[0][0]        \n",
      "                                                                 conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 8, 8, 64)     256         concatenate_12[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 8, 8, 64)     0           batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 8, 8, 32)     18432       activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_13 (Concatenate)    (None, 8, 8, 96)     0           concatenate_12[0][0]             \n",
      "                                                                 conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 8, 8, 96)     384         concatenate_13[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 8, 8, 96)     0           batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, 8, 8, 32)     27648       activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_14 (Concatenate)    (None, 8, 8, 128)    0           concatenate_13[0][0]             \n",
      "                                                                 conv2d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 8, 8, 128)    512         concatenate_14[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 8, 8, 128)    0           batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)              (None, 8, 8, 32)     36864       activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_15 (Concatenate)    (None, 8, 8, 160)    0           concatenate_14[0][0]             \n",
      "                                                                 conv2d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_18 (BatchNo (None, 8, 8, 160)    640         concatenate_15[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, 8, 8, 160)    0           batch_normalization_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_19 (Conv2D)              (None, 8, 8, 32)     46080       activation_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_16 (Concatenate)    (None, 8, 8, 192)    0           concatenate_15[0][0]             \n",
      "                                                                 conv2d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_19 (BatchNo (None, 8, 8, 192)    768         concatenate_16[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_19 (Activation)      (None, 8, 8, 192)    0           batch_normalization_19[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_20 (Conv2D)              (None, 8, 8, 32)     55296       activation_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_17 (Concatenate)    (None, 8, 8, 224)    0           concatenate_16[0][0]             \n",
      "                                                                 conv2d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_20 (BatchNo (None, 8, 8, 224)    896         concatenate_17[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_20 (Activation)      (None, 8, 8, 224)    0           batch_normalization_20[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_21 (Conv2D)              (None, 8, 8, 32)     7168        activation_20[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_2 (AveragePoo (None, 4, 4, 32)     0           conv2d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_21 (BatchNo (None, 4, 4, 32)     128         average_pooling2d_2[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_21 (Activation)      (None, 4, 4, 32)     0           batch_normalization_21[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_22 (Conv2D)              (None, 4, 4, 32)     9216        activation_21[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_18 (Concatenate)    (None, 4, 4, 64)     0           average_pooling2d_2[0][0]        \n",
      "                                                                 conv2d_22[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_22 (BatchNo (None, 4, 4, 64)     256         concatenate_18[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_22 (Activation)      (None, 4, 4, 64)     0           batch_normalization_22[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_23 (Conv2D)              (None, 4, 4, 32)     18432       activation_22[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_19 (Concatenate)    (None, 4, 4, 96)     0           concatenate_18[0][0]             \n",
      "                                                                 conv2d_23[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_23 (BatchNo (None, 4, 4, 96)     384         concatenate_19[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_23 (Activation)      (None, 4, 4, 96)     0           batch_normalization_23[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_24 (Conv2D)              (None, 4, 4, 32)     27648       activation_23[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_20 (Concatenate)    (None, 4, 4, 128)    0           concatenate_19[0][0]             \n",
      "                                                                 conv2d_24[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_24 (BatchNo (None, 4, 4, 128)    512         concatenate_20[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_24 (Activation)      (None, 4, 4, 128)    0           batch_normalization_24[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_25 (Conv2D)              (None, 4, 4, 32)     36864       activation_24[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_21 (Concatenate)    (None, 4, 4, 160)    0           concatenate_20[0][0]             \n",
      "                                                                 conv2d_25[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_25 (BatchNo (None, 4, 4, 160)    640         concatenate_21[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_25 (Activation)      (None, 4, 4, 160)    0           batch_normalization_25[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_26 (Conv2D)              (None, 4, 4, 32)     46080       activation_25[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_22 (Concatenate)    (None, 4, 4, 192)    0           concatenate_21[0][0]             \n",
      "                                                                 conv2d_26[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_26 (BatchNo (None, 4, 4, 192)    768         concatenate_22[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_26 (Activation)      (None, 4, 4, 192)    0           batch_normalization_26[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_27 (Conv2D)              (None, 4, 4, 32)     55296       activation_26[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_23 (Concatenate)    (None, 4, 4, 224)    0           concatenate_22[0][0]             \n",
      "                                                                 conv2d_27[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_27 (BatchNo (None, 4, 4, 224)    896         concatenate_23[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_27 (Activation)      (None, 4, 4, 224)    0           batch_normalization_27[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_3 (AveragePoo (None, 2, 2, 224)    0           activation_27[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_28 (Conv2D)              (None, 2, 2, 32)     7168        average_pooling2d_3[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_29 (Conv2D)              (None, 2, 2, 10)     330         conv2d_28[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d (Globa (None, 10)           0           conv2d_29[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_28 (Activation)      (None, 10)           0           global_average_pooling2d[0][0]   \n",
      "==================================================================================================\n",
      "Total params: 876,426\n",
      "Trainable params: 868,810\n",
      "Non-trainable params: 7,616\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Model(inputs=[input], outputs=[output])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aXi9TYkltVGh"
   },
   "outputs": [],
   "source": [
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "\n",
    "mean = X_train.mean(0)\n",
    "std = X_train.std(0)\n",
    "def preprocess_data(data_set):\n",
    "    # mean = np.array([125.3, 123.0, 113.9])\n",
    "    # std = np.array([63.0, 62.1, 66.7])\n",
    "\n",
    "    data_set -= mean\n",
    "    data_set /= std\n",
    "    return data_set\n",
    "\n",
    "X_train = preprocess_data(X_train)\n",
    "X_test = preprocess_data(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    },
    "colab_type": "code",
    "id": "mGQJ3El4tVGm",
    "outputId": "2d6490a3-64b6-4baa-e972-20e7596d70fe"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Data augementation\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "datagen_train = ImageDataGenerator(\n",
    "    width_shift_range=0.125,\n",
    "    height_shift_range=0.125,\n",
    "    horizontal_flip=True,\n",
    ")\n",
    "\n",
    "datagen_train.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FmFfA-gntVG4"
   },
   "outputs": [],
   "source": [
    "checkpoint_3 = ModelCheckpoint(\"model_dense.h5\",monitor=\"val_acc\",mode=\"max\",save_best_only = True,verbose=1) \n",
    "NAME = 'model_dense' \n",
    "tensorboard_2 = TensorBoard(log_dir='logss\\{}'.format(NAME),update_freq='epoch',batch_size=batch_size) \n",
    "callbacks_2 = [tensorboard_2,checkpoint_3]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "b4XOsW3ahSkL"
   },
   "outputs": [],
   "source": [
    "# determine Loss function and Optimizer\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=Adam(),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "gTE2UNamtVG_",
    "outputId": "af60e3a2-c6f2-4e1e-c4c3-d9c62a6a560a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/35\n",
      "1953/1953 [============================>.] - ETA: 0s - loss: 0.8582 - acc: 0.6952Epoch 1/35\n",
      "10000/1953 [=========================================================================================================================================================] - 10s 968us/sample - loss: 0.7712 - acc: 0.7534\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.75340, saving model to model_dense.h5\n",
      "1954/1953 [==============================] - 829s 424ms/step - loss: 0.8581 - acc: 0.6952 - val_loss: 0.7602 - val_acc: 0.7534\n",
      "Epoch 2/35\n",
      "1953/1953 [============================>.] - ETA: 0s - loss: 0.4530 - acc: 0.8430Epoch 1/35\n",
      "10000/1953 [=========================================================================================================================================================] - 9s 855us/sample - loss: 0.5275 - acc: 0.7822\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.75340 to 0.78220, saving model to model_dense.h5\n",
      "1954/1953 [==============================] - 813s 416ms/step - loss: 0.4530 - acc: 0.8430 - val_loss: 0.6827 - val_acc: 0.7822\n",
      "Epoch 3/35\n",
      "1953/1953 [============================>.] - ETA: 0s - loss: 0.3410 - acc: 0.8811Epoch 1/35\n",
      "10000/1953 [=========================================================================================================================================================] - 9s 857us/sample - loss: 0.3822 - acc: 0.8438\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.78220 to 0.84380, saving model to model_dense.h5\n",
      "1954/1953 [==============================] - 811s 415ms/step - loss: 0.3410 - acc: 0.8811 - val_loss: 0.4823 - val_acc: 0.8438\n",
      "Epoch 4/35\n",
      "1953/1953 [============================>.] - ETA: 0s - loss: 0.2758 - acc: 0.9040Epoch 1/35\n",
      "10000/1953 [=========================================================================================================================================================] - 9s 856us/sample - loss: 0.3995 - acc: 0.8614\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.84380 to 0.86140, saving model to model_dense.h5\n",
      "1954/1953 [==============================] - 814s 417ms/step - loss: 0.2758 - acc: 0.9040 - val_loss: 0.4315 - val_acc: 0.8614\n",
      "Epoch 5/35\n",
      "1953/1953 [============================>.] - ETA: 0s - loss: 0.2279 - acc: 0.9203Epoch 1/35\n",
      "10000/1953 [=========================================================================================================================================================] - 9s 857us/sample - loss: 0.3589 - acc: 0.8584\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.86140\n",
      "1954/1953 [==============================] - 814s 417ms/step - loss: 0.2278 - acc: 0.9202 - val_loss: 0.4558 - val_acc: 0.8584\n",
      "Epoch 6/35\n",
      "1953/1953 [============================>.] - ETA: 0s - loss: 0.1930 - acc: 0.9320Epoch 1/35\n",
      "10000/1953 [=========================================================================================================================================================] - 9s 855us/sample - loss: 0.5857 - acc: 0.8447\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.86140\n",
      "1954/1953 [==============================] - 812s 416ms/step - loss: 0.1930 - acc: 0.9320 - val_loss: 0.5696 - val_acc: 0.8447\n",
      "Epoch 7/35\n",
      "1953/1953 [============================>.] - ETA: 0s - loss: 0.1652 - acc: 0.9420Epoch 1/35\n",
      "10000/1953 [=========================================================================================================================================================] - 9s 859us/sample - loss: 0.3140 - acc: 0.8722\n",
      "\n",
      "Epoch 00007: val_acc improved from 0.86140 to 0.87220, saving model to model_dense.h5\n",
      "1954/1953 [==============================] - 811s 415ms/step - loss: 0.1652 - acc: 0.9420 - val_loss: 0.4294 - val_acc: 0.8722\n",
      "Epoch 8/35\n",
      "1953/1953 [============================>.] - ETA: 0s - loss: 0.1444 - acc: 0.9483Epoch 1/35\n",
      "10000/1953 [=========================================================================================================================================================] - 9s 856us/sample - loss: 0.4902 - acc: 0.8596\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.87220\n",
      "1954/1953 [==============================] - 814s 417ms/step - loss: 0.1444 - acc: 0.9483 - val_loss: 0.5420 - val_acc: 0.8596\n",
      "Epoch 9/35\n",
      "1953/1953 [============================>.] - ETA: 0s - loss: 0.1259 - acc: 0.9553Epoch 1/35\n",
      "10000/1953 [=========================================================================================================================================================] - 9s 857us/sample - loss: 0.5188 - acc: 0.8702\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.87220\n",
      "1954/1953 [==============================] - 817s 418ms/step - loss: 0.1259 - acc: 0.9552 - val_loss: 0.5179 - val_acc: 0.8702\n",
      "Epoch 10/35\n",
      "1953/1953 [============================>.] - ETA: 0s - loss: 0.1111 - acc: 0.9606Epoch 1/35\n",
      "10000/1953 [=========================================================================================================================================================] - 9s 858us/sample - loss: 0.5359 - acc: 0.8763\n",
      "\n",
      "Epoch 00010: val_acc improved from 0.87220 to 0.87630, saving model to model_dense.h5\n",
      "1954/1953 [==============================] - 815s 417ms/step - loss: 0.1111 - acc: 0.9606 - val_loss: 0.4730 - val_acc: 0.8763\n",
      "Epoch 11/35\n",
      "1953/1953 [============================>.] - ETA: 0s - loss: 0.0982 - acc: 0.9651Epoch 1/35\n",
      "10000/1953 [=========================================================================================================================================================] - 9s 855us/sample - loss: 0.2809 - acc: 0.8931\n",
      "\n",
      "Epoch 00011: val_acc improved from 0.87630 to 0.89310, saving model to model_dense.h5\n",
      "1954/1953 [==============================] - 813s 416ms/step - loss: 0.0982 - acc: 0.9651 - val_loss: 0.4108 - val_acc: 0.8931\n",
      "Epoch 12/35\n",
      "1953/1953 [============================>.] - ETA: 0s - loss: 0.0885 - acc: 0.9682Epoch 1/35\n",
      "10000/1953 [=========================================================================================================================================================] - 9s 855us/sample - loss: 0.5778 - acc: 0.8650\n",
      "\n",
      "Epoch 00012: val_acc did not improve from 0.89310\n",
      "1954/1953 [==============================] - 817s 418ms/step - loss: 0.0885 - acc: 0.9682 - val_loss: 0.5875 - val_acc: 0.8650\n",
      "Epoch 13/35\n",
      "1953/1953 [============================>.] - ETA: 0s - loss: 0.0799 - acc: 0.9715Epoch 1/35\n",
      "10000/1953 [=========================================================================================================================================================] - 9s 868us/sample - loss: 0.3935 - acc: 0.8876\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.89310\n",
      "1954/1953 [==============================] - 817s 418ms/step - loss: 0.0799 - acc: 0.9715 - val_loss: 0.4731 - val_acc: 0.8876\n",
      "Epoch 14/35\n",
      "1953/1953 [============================>.] - ETA: 0s - loss: 0.0735 - acc: 0.9738Epoch 1/35\n",
      "10000/1953 [=========================================================================================================================================================] - 9s 858us/sample - loss: 0.3456 - acc: 0.8898\n",
      "\n",
      "Epoch 00014: val_acc did not improve from 0.89310\n",
      "1954/1953 [==============================] - 814s 416ms/step - loss: 0.0735 - acc: 0.9738 - val_loss: 0.4670 - val_acc: 0.8898\n",
      "Epoch 15/35\n",
      "1953/1953 [============================>.] - ETA: 0s - loss: 0.0683 - acc: 0.9760Epoch 1/35\n",
      "10000/1953 [=========================================================================================================================================================] - 9s 858us/sample - loss: 0.7455 - acc: 0.8888\n",
      "\n",
      "Epoch 00015: val_acc did not improve from 0.89310\n",
      "1954/1953 [==============================] - 812s 416ms/step - loss: 0.0683 - acc: 0.9760 - val_loss: 0.5049 - val_acc: 0.8888\n",
      "Epoch 16/35\n",
      "1953/1953 [============================>.] - ETA: 0s - loss: 0.0623 - acc: 0.9780Epoch 1/35\n",
      "10000/1953 [=========================================================================================================================================================] - 9s 860us/sample - loss: 0.3478 - acc: 0.8793\n",
      "\n",
      "Epoch 00016: val_acc did not improve from 0.89310\n",
      "1954/1953 [==============================] - 814s 417ms/step - loss: 0.0623 - acc: 0.9780 - val_loss: 0.5715 - val_acc: 0.8793\n",
      "Epoch 17/35\n",
      "1953/1953 [============================>.] - ETA: 0s - loss: 0.0583 - acc: 0.9788Epoch 1/35\n",
      "10000/1953 [=========================================================================================================================================================] - 9s 861us/sample - loss: 0.3910 - acc: 0.8963\n",
      "\n",
      "Epoch 00017: val_acc improved from 0.89310 to 0.89630, saving model to model_dense.h5\n",
      "1954/1953 [==============================] - 818s 418ms/step - loss: 0.0583 - acc: 0.9788 - val_loss: 0.4818 - val_acc: 0.8963\n",
      "Epoch 18/35\n",
      "1953/1953 [============================>.] - ETA: 0s - loss: 0.0545 - acc: 0.9807Epoch 1/35\n",
      "10000/1953 [=========================================================================================================================================================] - 9s 859us/sample - loss: 0.7254 - acc: 0.8965\n",
      "\n",
      "Epoch 00018: val_acc improved from 0.89630 to 0.89650, saving model to model_dense.h5\n",
      "1954/1953 [==============================] - 814s 417ms/step - loss: 0.0545 - acc: 0.9807 - val_loss: 0.4743 - val_acc: 0.8965\n",
      "Epoch 19/35\n",
      "1953/1953 [============================>.] - ETA: 0s - loss: 0.0504 - acc: 0.9821Epoch 1/35\n",
      "10000/1953 [=========================================================================================================================================================] - 9s 861us/sample - loss: 0.3733 - acc: 0.8976\n",
      "\n",
      "Epoch 00019: val_acc improved from 0.89650 to 0.89760, saving model to model_dense.h5\n",
      "1954/1953 [==============================] - 814s 417ms/step - loss: 0.0504 - acc: 0.9821 - val_loss: 0.5073 - val_acc: 0.8976\n",
      "Epoch 20/35\n",
      "1953/1953 [============================>.] - ETA: 0s - loss: 0.0488 - acc: 0.9827Epoch 1/35\n",
      "10000/1953 [=========================================================================================================================================================] - 9s 859us/sample - loss: 0.3946 - acc: 0.9055\n",
      "\n",
      "Epoch 00020: val_acc improved from 0.89760 to 0.90550, saving model to model_dense.h5\n",
      "1954/1953 [==============================] - 816s 418ms/step - loss: 0.0488 - acc: 0.9827 - val_loss: 0.4279 - val_acc: 0.9055\n",
      "Epoch 21/35\n",
      "1953/1953 [============================>.] - ETA: 0s - loss: 0.0455 - acc: 0.9840Epoch 1/35\n",
      "10000/1953 [=========================================================================================================================================================] - 9s 858us/sample - loss: 0.4234 - acc: 0.9008\n",
      "\n",
      "Epoch 00021: val_acc did not improve from 0.90550\n",
      "1954/1953 [==============================] - 816s 418ms/step - loss: 0.0455 - acc: 0.9839 - val_loss: 0.4930 - val_acc: 0.9008\n",
      "Epoch 22/35\n",
      "1953/1953 [============================>.] - ETA: 0s - loss: 0.0438 - acc: 0.9847Epoch 1/35\n",
      "10000/1953 [=========================================================================================================================================================] - 9s 867us/sample - loss: 0.4884 - acc: 0.9041\n",
      "\n",
      "Epoch 00022: val_acc did not improve from 0.90550\n",
      "1954/1953 [==============================] - 814s 417ms/step - loss: 0.0439 - acc: 0.9847 - val_loss: 0.4589 - val_acc: 0.9041\n",
      "Epoch 23/35\n",
      "1953/1953 [============================>.] - ETA: 0s - loss: 0.0413 - acc: 0.9855Epoch 1/35\n",
      "10000/1953 [=========================================================================================================================================================] - 9s 855us/sample - loss: 0.5515 - acc: 0.8945\n",
      "\n",
      "Epoch 00023: val_acc did not improve from 0.90550\n",
      "1954/1953 [==============================] - 812s 416ms/step - loss: 0.0413 - acc: 0.9855 - val_loss: 0.5492 - val_acc: 0.8945\n",
      "Epoch 24/35\n",
      "1953/1953 [============================>.] - ETA: 0s - loss: 0.0395 - acc: 0.9862Epoch 1/35\n",
      "10000/1953 [=========================================================================================================================================================] - 9s 861us/sample - loss: 0.7714 - acc: 0.8984\n",
      "\n",
      "Epoch 00024: val_acc did not improve from 0.90550\n",
      "1954/1953 [==============================] - 815s 417ms/step - loss: 0.0395 - acc: 0.9862 - val_loss: 0.5492 - val_acc: 0.8984\n",
      "Epoch 25/35\n",
      "1953/1953 [============================>.] - ETA: 0s - loss: 0.0383 - acc: 0.9865Epoch 1/35\n",
      "10000/1953 [=========================================================================================================================================================] - 9s 857us/sample - loss: 0.5539 - acc: 0.9007\n",
      "\n",
      "Epoch 00025: val_acc did not improve from 0.90550\n",
      "1954/1953 [==============================] - 816s 418ms/step - loss: 0.0382 - acc: 0.9865 - val_loss: 0.4738 - val_acc: 0.9007\n",
      "Epoch 26/35\n",
      "1953/1953 [============================>.] - ETA: 0s - loss: 0.0363 - acc: 0.9874Epoch 1/35\n",
      "10000/1953 [=========================================================================================================================================================] - 9s 854us/sample - loss: 0.5164 - acc: 0.9046\n",
      "\n",
      "Epoch 00026: val_acc did not improve from 0.90550\n",
      "1954/1953 [==============================] - 814s 417ms/step - loss: 0.0363 - acc: 0.9874 - val_loss: 0.4864 - val_acc: 0.9046\n",
      "Epoch 27/35\n",
      "1953/1953 [============================>.] - ETA: 0s - loss: 0.0337 - acc: 0.9883Epoch 1/35\n",
      "10000/1953 [=========================================================================================================================================================] - 9s 858us/sample - loss: 0.6035 - acc: 0.8855\n",
      "\n",
      "Epoch 00027: val_acc did not improve from 0.90550\n",
      "1954/1953 [==============================] - 813s 416ms/step - loss: 0.0337 - acc: 0.9883 - val_loss: 0.6773 - val_acc: 0.8855\n",
      "Epoch 28/35\n",
      "1953/1953 [============================>.] - ETA: 0s - loss: 0.0338 - acc: 0.9879Epoch 1/35\n",
      "10000/1953 [=========================================================================================================================================================] - 9s 861us/sample - loss: 0.5395 - acc: 0.8857\n",
      "\n",
      "Epoch 00028: val_acc did not improve from 0.90550\n",
      "1954/1953 [==============================] - 816s 418ms/step - loss: 0.0338 - acc: 0.9879 - val_loss: 0.5979 - val_acc: 0.8857\n",
      "Epoch 29/35\n",
      "1953/1953 [============================>.] - ETA: 0s - loss: 0.0319 - acc: 0.9889Epoch 1/35\n",
      "10000/1953 [=========================================================================================================================================================] - 9s 857us/sample - loss: 0.3917 - acc: 0.8982\n",
      "\n",
      "Epoch 00029: val_acc did not improve from 0.90550\n",
      "1954/1953 [==============================] - 817s 418ms/step - loss: 0.0319 - acc: 0.9889 - val_loss: 0.5456 - val_acc: 0.8982\n",
      "Epoch 30/35\n",
      "1771/1953 [==========================>...] - ETA: 1:15 - loss: 0.0311 - acc: 0.9890Buffered data was truncated after reaching the output size limit."
     ]
    }
   ],
   "source": [
    "history = model.fit_generator(datagen_train.flow(X_train, y_train, batch_size=batch_size),steps_per_epoch=(len(X_train)/batch_size)*5,\n",
    "    epochs=epochs,\n",
    "    verbose = 1,\n",
    "    validation_data=(X_test, y_test),\n",
    "    callbacks = callbacks_2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 67
    },
    "colab_type": "code",
    "id": "ZcWydmIVhZGr",
    "outputId": "5f6367c5-3665-4f8e-87ad-62b25056f02d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 11s 1ms/sample - loss: 0.4859 - acc: 0.9141\n",
      "Test loss: 0.4859287844727747\n",
      "Test accuracy: 0.9141\n"
     ]
    }
   ],
   "source": [
    "# Test the model\n",
    "score = model.evaluate(X_test, y_test, verbose=1)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    },
    "colab_type": "code",
    "id": "UE3lF6EH1r_L",
    "outputId": "3de3727a-8469-4442-afad-992c50003dbf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "# Save the trained weights in to .h5 format\n",
    "model.save_weights(\"DNST_model.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vWcOD2foFSnX"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of DenseNet - cifar10 - Copy.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
